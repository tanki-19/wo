{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":39911,"sourceType":"datasetVersion","datasetId":31296}],"dockerImageVersionId":29282,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"72d99e7f-1a25-4acd-9860-f948b903e746","_cell_guid":"44efda10-a131-42be-847a-a32c8e2307a4","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torchvision.utils import make_grid\nimport torchvision.utils as vutils\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport copy\nimport time,os\nimport cv2 as cv\nfrom tqdm import tqdm_notebook as tqdm\nimport matplotlib.image as mpimg\n\nimport torchvision.transforms.functional as TF","metadata":{"_uuid":"013e2bf2-c5ae-4028-b39b-fcd36cacf137","_cell_guid":"835bd7da-5bfc-486e-b407-26a898bea477","collapsed":false,"execution":{"iopub.status.busy":"2024-03-02T14:30:03.511135Z","iopub.execute_input":"2024-03-02T14:30:03.511442Z","iopub.status.idle":"2024-03-02T14:30:03.519686Z","shell.execute_reply.started":"2024-03-02T14:30:03.511399Z","shell.execute_reply":"2024-03-02T14:30:03.518730Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Code -- https://github.com/alexandru-dinu/cae\n# DataBase -- https://www.kaggle.com/hsankesara/flickr-image-dataset\n\n\n\n\nimg_dir = '/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/flickr30k_images/'\nimg_list = os.listdir(img_dir)\nprint(len(img_list))\nvalid_ratio = 0.8","metadata":{"_uuid":"c7a80a7c-f6b0-4d3b-8602-e5357d3c73c8","_cell_guid":"b8ce7263-4402-4552-bf9b-492816dd90de","collapsed":false,"execution":{"iopub.status.busy":"2024-03-02T14:30:08.171944Z","iopub.execute_input":"2024-03-02T14:30:08.172259Z","iopub.status.idle":"2024-03-02T14:30:08.931957Z","shell.execute_reply.started":"2024-03-02T14:30:08.172218Z","shell.execute_reply":"2024-03-02T14:30:08.931081Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ImageData(Dataset):\n    def __init__(self,is_train=True):\n        self.is_train = is_train\n        self.transform = transforms.Compose([transforms.ToTensor(),])\n        self.train_index = int(valid_ratio * len(img_list))\n        self.crop = transforms.CenterCrop((218,178))\n    def __len__(self):\n        if self.is_train:\n            return self.train_index\n        else:\n            return len(img_list) - self.train_index -1\n    def __getitem__(self, index):\n        if not self.is_train:\n            index = self.train_index + index\n#         print(\"hey  \"*4 + str(index))\n        img = mpimg.imread(img_dir+img_list[index])\n        img = self.crop(TF.to_pil_image(img))\n        img = self.transform(img)\n        img = (img-0.5) /0.5\n#         img = (img - 255.0) / 255.0\n        return img","metadata":{"_uuid":"3ef8273e-0c84-43e0-9a09-af0db38b8ec8","_cell_guid":"15759067-c847-4ab7-87c4-df97aae18731","collapsed":false,"execution":{"iopub.status.busy":"2024-03-02T14:30:15.236986Z","iopub.execute_input":"2024-03-02T14:30:15.237302Z","iopub.status.idle":"2024-03-02T14:30:15.246828Z","shell.execute_reply.started":"2024-03-02T14:30:15.237255Z","shell.execute_reply":"2024-03-02T14:30:15.246019Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size=20\ndataset = ImageData()\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\ndevice = 'cuda'","metadata":{"_uuid":"4dd41d83-3618-4893-a40a-c9a39f21f079","_cell_guid":"3a371f21-c7fe-4eee-9f9e-e906595f89d9","collapsed":false,"execution":{"iopub.status.busy":"2024-03-02T14:30:18.073211Z","iopub.execute_input":"2024-03-02T14:30:18.073569Z","iopub.status.idle":"2024-03-02T14:30:18.078356Z","shell.execute_reply.started":"2024-03-02T14:30:18.073509Z","shell.execute_reply":"2024-03-02T14:30:18.077496Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = next(iter(dataloader))\nprint(a[0].shape)\nimg = a[15]\nimg = img *0.5 + 0.5\nplt.imshow(img.permute(1,2,0))","metadata":{"_uuid":"d4821e7d-2df9-4223-97b2-ed520517e835","_cell_guid":"333f85b6-b29c-4c5d-aa22-c2df26b48916","collapsed":false,"execution":{"iopub.status.busy":"2024-03-02T14:30:19.009493Z","iopub.execute_input":"2024-03-02T14:30:19.009832Z","iopub.status.idle":"2024-03-02T14:30:19.757256Z","shell.execute_reply.started":"2024-03-02T14:30:19.009765Z","shell.execute_reply":"2024-03-02T14:30:19.756036Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# custom weights initialization called on netG and netD\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)","metadata":{"_uuid":"371d9c98-1de3-45f4-92ac-373087137e48","_cell_guid":"63b3e42a-3ce9-4119-a7db-487f186dd6b1","collapsed":false,"execution":{"iopub.status.busy":"2024-03-02T14:30:24.855235Z","iopub.execute_input":"2024-03-02T14:30:24.855632Z","iopub.status.idle":"2024-03-02T14:30:24.862447Z","shell.execute_reply.started":"2024-03-02T14:30:24.855567Z","shell.execute_reply":"2024-03-02T14:30:24.861218Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMG_WIDTH = 178\nIMG_HEIGHT = 218\nlatent_size = 200\n\nnum_channels_in_encoder = 8","metadata":{"_uuid":"4dff9210-0daf-4e21-8211-9f1a8fed6e23","_cell_guid":"847b5846-d0b2-4b33-ba69-d47e28a2a763","collapsed":false,"execution":{"iopub.status.busy":"2024-03-02T14:30:26.156300Z","iopub.execute_input":"2024-03-02T14:30:26.156726Z","iopub.status.idle":"2024-03-02T14:30:26.161130Z","shell.execute_reply.started":"2024-03-02T14:30:26.156636Z","shell.execute_reply":"2024-03-02T14:30:26.160253Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encoder Model\nclass Encoder(nn.Module):\n    def __init__(self):\n        super(Encoder, self).__init__()\n        \n        # ENCODER\n\n        # 64x64x64\n        self.e_conv_1 = nn.Sequential(\n            nn.ZeroPad2d((1, 2, 1, 2)),\n            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(5, 5), stride=(2, 2)),nn.LeakyReLU()\n        )\n\n        # 128x32x32\n        self.e_conv_2 = nn.Sequential(\n            nn.ZeroPad2d((1, 2, 1, 2)),\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(5, 5), stride=(2, 2)),\n            nn.LeakyReLU()\n        )\n        \n        # 128x32x32\n        self.e_block_1 = nn.Sequential(\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n            nn.LeakyReLU(),\n\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n        )\n\n        # 128x32x32\n        self.e_block_2 = nn.Sequential(\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n            nn.LeakyReLU(),\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n        )\n\n        # 128x32x32\n        self.e_block_3 = nn.Sequential(\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n            nn.LeakyReLU(),\n\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n        )\n\n        # 32x32x32\n        self.e_conv_3 = nn.Sequential(\n            nn.Conv2d(in_channels=128, out_channels=num_channels_in_encoder, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)),\n            nn.Tanh()\n        )\n    def forward(self, x):\n        ec1 = self.e_conv_1(x)\n        ec2 = self.e_conv_2(ec1)\n        eblock1 = self.e_block_1(ec2) + ec2\n        eblock2 = self.e_block_2(eblock1) + eblock1\n        eblock3 = self.e_block_3(eblock2) + eblock2\n        ec3 = self.e_conv_3(eblock3)  # in [-1, 1] from tanh activation\n        return ec3","metadata":{"_uuid":"28ae86cb-3750-4485-9729-4ccddc967ef1","_cell_guid":"23e4c77e-54e6-43fa-afd2-a7d085434875","collapsed":false,"execution":{"iopub.status.busy":"2024-03-02T14:30:26.655012Z","iopub.execute_input":"2024-03-02T14:30:26.655328Z","iopub.status.idle":"2024-03-02T14:30:26.677531Z","shell.execute_reply.started":"2024-03-02T14:30:26.655280Z","shell.execute_reply":"2024-03-02T14:30:26.676758Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device","metadata":{"_uuid":"ca647305-191b-4f4c-aa9f-9a8ce3beb8ed","_cell_guid":"b24ee61c-e51a-4556-9d8c-e60297d9a023","collapsed":false,"execution":{"iopub.status.busy":"2024-03-02T14:30:28.372456Z","iopub.execute_input":"2024-03-02T14:30:28.372749Z","iopub.status.idle":"2024-03-02T14:30:28.377818Z","shell.execute_reply.started":"2024-03-02T14:30:28.372705Z","shell.execute_reply":"2024-03-02T14:30:28.376939Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"netE = Encoder().to(device)\nnetE.apply(weights_init)\ninp = torch.randn(IMG_WIDTH*IMG_HEIGHT*3 * 100)\ninp = inp.view((-1,3,IMG_HEIGHT,IMG_WIDTH))\noutput = netE(inp.to(device))\nprint(output.shape)\nprint('The Compression Ratio is :  ' + str((output.shape[1]*output.shape[2]*output.shape[3])/(IMG_WIDTH*IMG_HEIGHT*3)*100) )","metadata":{"_uuid":"04bd6acd-5422-44e8-bb43-c4245730f898","_cell_guid":"9dbb59a7-50dd-4631-8ae3-bd3b4bc3969f","collapsed":false,"execution":{"iopub.status.busy":"2024-03-02T14:30:29.250824Z","iopub.execute_input":"2024-03-02T14:30:29.251181Z","iopub.status.idle":"2024-03-02T14:30:33.816940Z","shell.execute_reply.started":"2024-03-02T14:30:29.251117Z","shell.execute_reply":"2024-03-02T14:30:33.816006Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generator / Decoder Model\n\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        \n        # DECODER\n#         self.latent_fc1 = nn.Sequential(\n#             nn.Linear(latent_size,1000),\n#             nn.Sigmoid(),\n#         )\n#         self.latent_fc2 = nn.Sequential(\n#             nn.Linear(1000,54*44),\n#             nn.Sigmoid(),\n#         )\n        # 128x64x64\n        self.d_up_conv_1 = nn.Sequential(\n        nn.Conv2d(in_channels=num_channels_in_encoder, out_channels=64, kernel_size=(3, 3), stride=(1, 1)),\n            nn.LeakyReLU(),\n\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.ConvTranspose2d(in_channels=64, out_channels=128, kernel_size=(2, 2), stride=(2, 2))\n        )\n\n        # 128x64x64\n        self.d_block_1 = nn.Sequential(\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n            nn.LeakyReLU(),\n\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n        )\n\n        # 128x64x64\n        self.d_block_2 = nn.Sequential(\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n            nn.LeakyReLU(),\n\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n        )\n\n        # 128x64x64\n        self.d_block_3 = nn.Sequential(\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n            nn.LeakyReLU(),\n\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n        )\n\n        # 256x128x128\n        self.d_up_conv_2 = nn.Sequential(\n            nn.Conv2d(in_channels=128, out_channels=32, kernel_size=(3, 3), stride=(1, 1)),\n            nn.LeakyReLU(),\n\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.ConvTranspose2d(in_channels=32, out_channels=256, kernel_size=(2, 2), stride=(2, 2))\n        )\n\n        # 3x128x128\n        self.d_up_conv_3 = nn.Sequential(\n            nn.Conv2d(in_channels=256, out_channels=16, kernel_size=(3, 3), stride=(1, 1)),\n            nn.LeakyReLU(),\n\n            nn.ReflectionPad2d((3, 3, 3, 3)),\n            nn.Conv2d(in_channels=16, out_channels=3, kernel_size=(3, 3), stride=(1, 1)),\n            nn.Tanh()\n        )\n\n        \n        \n    def forward(self, x):\n        uc1 = self.d_up_conv_1(x)\n        dblock1 = self.d_block_1(uc1) + uc1\n        dblock2 = self.d_block_2(dblock1) + dblock1\n        dblock3 = self.d_block_3(dblock2) + dblock2\n        uc2 = self.d_up_conv_2(dblock3)\n        dec = self.d_up_conv_3(uc2)\n        return dec","metadata":{"_uuid":"65633225-2e2e-4a5d-ad49-f04094c462c2","_cell_guid":"5e9d4d5b-382d-4a40-bda1-71f31d44faf0","collapsed":false,"execution":{"iopub.status.busy":"2024-03-02T14:30:35.811521Z","iopub.execute_input":"2024-03-02T14:30:35.811814Z","iopub.status.idle":"2024-03-02T14:30:35.838400Z","shell.execute_reply.started":"2024-03-02T14:30:35.811764Z","shell.execute_reply":"2024-03-02T14:30:35.837619Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"netG = Generator().to(device)\nnetG.apply(weights_init)\ninp = torch.randn(100*num_channels_in_encoder*54*44).view((-1,num_channels_in_encoder,54,44)).to(device)\noutput = netG(inp)\nprint(output.shape)\n#218 * 178","metadata":{"_uuid":"134c8c5d-e45e-47b2-9587-782e5f6b4645","_cell_guid":"775a6327-c0b0-4b30-8cc0-f451fc04479e","collapsed":false,"execution":{"iopub.status.busy":"2024-03-02T14:30:57.899134Z","iopub.execute_input":"2024-03-02T14:30:57.899435Z","iopub.status.idle":"2024-03-02T14:30:57.955852Z","shell.execute_reply.started":"2024-03-02T14:30:57.899391Z","shell.execute_reply":"2024-03-02T14:30:57.954821Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"_uuid":"eb1ca2c8-50c1-4466-938d-4e0bcb2a2ce0","_cell_guid":"dc9cec0f-91d3-4beb-bdc5-ca5ef11c049f","collapsed":false,"execution":{"iopub.status.busy":"2024-03-02T14:30:58.933783Z","iopub.execute_input":"2024-03-02T14:30:58.934140Z","iopub.status.idle":"2024-03-02T14:30:58.998103Z","shell.execute_reply.started":"2024-03-02T14:30:58.934074Z","shell.execute_reply":"2024-03-02T14:30:58.997151Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Discriminator Model\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.latent_layer1 = nn.Sequential(\n            nn.ConvTranspose2d(num_channels_in_encoder, 12, (3,3), stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n        )\n        self.latent_layer2 = nn.Sequential(\n            nn.ConvTranspose2d(12, 16, (3,3), stride=1, padding=2, output_padding=0, groups=1, bias=True, dilation=1),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n        )\n        self.latent_layer3 = nn.Sequential(\n            nn.ConvTranspose2d(16, 24, (3,3), stride=2, padding=2, output_padding=1, groups=1, bias=True, dilation=1),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n        )\n        self.latent_layer4 = nn.Sequential(\n            nn.ConvTranspose2d(24, 36, (5,5), stride=2, padding=0, output_padding=1, groups=1, bias=True, dilation=1),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n        )\n        self.latent_layer5 = nn.Sequential(\n            nn.ConvTranspose2d(36, 3, (3,3), stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n            nn.Tanh(),\n        )\n\n        \n        self.layer1 = nn.Sequential(\n            nn.Conv2d(in_channels=6, out_channels=64, kernel_size=3,stride = 1,padding=0),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n            nn.Dropout(0.3),\n        )\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5,stride = 2,padding=0),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n            nn.Dropout(0.3),\n        )\n        self.layer3 = nn.Sequential(\n            nn.Conv2d(in_channels=128, out_channels=32, kernel_size=3,stride = 2,padding=2),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n            nn.Dropout(0.3),\n        )\n        self.layer4 = nn.Sequential(\n            nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3,stride = 1,padding=2),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n            nn.Dropout(0.3),\n        )\n        self.layer5 = nn.Sequential(\n            nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3,stride = 1,padding=0),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n            nn.Dropout(0.3),\n            nn.Tanh(),\n        )\n        \n        \n        self.fc1 = nn.Sequential(\n            nn.Linear(8*54*44,2000),\n            nn.Sigmoid(),\n        )\n        \n        self.fc2 = nn.Sequential(\n            nn.Linear(2000,100),\n            nn.Sigmoid(),\n        )\n        self.fc3 = nn.Sequential(\n            nn.Linear(100,1),\n            nn.Sigmoid(),\n        )\n        \n        \n    def forward(self, x):\n        y = x['encoded'].to(device)\n        y = self.latent_layer1(y)\n        y = self.latent_layer2(y)\n        y = self.latent_layer3(y)\n        y = self.latent_layer4(y)\n        y = self.latent_layer5(y)\n#         print(y.shape)\n        x = x['img'].to(device)\n#         print(x.shape)\n        x = torch.cat((x,y),1)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.layer5(x)\n#         print(x.shape)\n        x= x.reshape((x.shape[0],-1))\n        x = self.fc1(x)\n        x = self.fc2(x)\n        x = self.fc3(x)\n        return x","metadata":{"_uuid":"ab487ae9-daa1-4b08-ba34-27db29b092e7","_cell_guid":"07132a08-fec0-4edf-afe7-05dd7dce0744","collapsed":false,"execution":{"iopub.status.busy":"2024-03-02T14:31:09.515727Z","iopub.execute_input":"2024-03-02T14:31:09.516064Z","iopub.status.idle":"2024-03-02T14:31:09.547905Z","shell.execute_reply.started":"2024-03-02T14:31:09.516008Z","shell.execute_reply":"2024-03-02T14:31:09.547182Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"netD = Discriminator().to(device)\nnetD.apply(weights_init)\ninp_x = {}\ninp_x['img']=torch.randn(IMG_WIDTH*IMG_HEIGHT*3 * 100).view((-1,3,IMG_HEIGHT,IMG_WIDTH))\ninp_x['encoded'] = torch.randn(100*num_channels_in_encoder*54*44).view((-1,num_channels_in_encoder,54,44))\noutput = netD(inp_x)\noutput.shape","metadata":{"_uuid":"6c4388c4-3077-411a-97b6-f1c7594d3f77","_cell_guid":"609b7e03-a685-4783-b7e0-d70305c26943","collapsed":false,"execution":{"iopub.status.busy":"2024-03-02T14:31:11.034110Z","iopub.execute_input":"2024-03-02T14:31:11.034409Z","iopub.status.idle":"2024-03-02T14:31:11.687697Z","shell.execute_reply.started":"2024-03-02T14:31:11.034364Z","shell.execute_reply":"2024-03-02T14:31:11.686886Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = 0.0002\n# Initialize BCELoss function\ncriterion = nn.BCELoss()\nmsecriterion = nn.MSELoss()\nl1criterion = nn.L1Loss()\n# Establish convention for real and fake labels during training\nreal_label = 1\nfake_label = 0\n\n# Setup Adam optimizers for both G and D\noptimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(0.5, 0.999))\noptimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999))\noptimizerE = optim.Adam(netE.parameters(), lr=lr, betas=(0.5, 0.999))","metadata":{"_uuid":"e0b19230-9c6c-44ce-8f8e-ab58b6e60107","_cell_guid":"19c63a1b-1e6d-4048-9398-546dedbdd485","collapsed":false,"execution":{"iopub.status.busy":"2024-03-02T14:31:12.734244Z","iopub.execute_input":"2024-03-02T14:31:12.734627Z","iopub.status.idle":"2024-03-02T14:31:12.748711Z","shell.execute_reply.started":"2024-03-02T14:31:12.734563Z","shell.execute_reply":"2024-03-02T14:31:12.747920Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_dataset = ImageData(is_train=False)\nnum_images_to_show = 1\nvalid_dataloader = DataLoader(valid_dataset, batch_size=num_images_to_show, shuffle=True)\nvalid_batch = next(iter(valid_dataloader)).to(device)","metadata":{"_uuid":"45f61d80-deb6-4ec6-9a84-322c8f6a48bc","_cell_guid":"424adc08-9f81-4915-a416-8d030d5d73e1","collapsed":false,"execution":{"iopub.status.busy":"2024-03-02T14:31:14.972567Z","iopub.execute_input":"2024-03-02T14:31:14.972859Z","iopub.status.idle":"2024-03-02T14:31:15.000049Z","shell.execute_reply.started":"2024-03-02T14:31:14.972815Z","shell.execute_reply":"2024-03-02T14:31:14.999307Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training Loop\n\n# Lists to keep track of progress\nG_losses = []\nD_losses = []\nE_losses = []\niters = 0\nnum_epochs = 9\n\nprint(\"Starting Training Loop...\")\n# For each epoch\nfor epoch in range(num_epochs):\n    # For each batch in the dataloader\n    for i, (images) in enumerate(dataloader, 0):\n        netG.train()\n        netD.train()\n        netE.train()\n        \n        netD.zero_grad()\n        \n        images = images.to(device)\n        fake_images = netG(netE(images))\n        ############################\n        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n        ###########################\n        \n        ## Create a fake pair batch --\n\n        inp_x = {}\n        inp_x['img']=images\n        inp_x['encoded'] = netE(images)\n        \n#         label = torch.full((images.size(0),), real_label, device=device)\n        label = torch.FloatTensor(np.random.uniform(low=0.855, high=0.999, size=(images.size(0)))).to(device)\n        output = netD(inp_x).view(-1)\n        errD_real = criterion(output, label)\n        errD_real.backward(retain_graph=True)\n        D_x = output.mean().item()\n        \n        inp_x_fake = {}\n        inp_x_fake['img']=fake_images\n        inp_x_fake['encoded'] = netE(images)\n        label = torch.FloatTensor(np.random.uniform(low=0.005, high=0.155, size=(images.size(0)))).to(device)\n#         label.fill_(fake_label)\n        output = netD(inp_x_fake).view(-1)\n        errD_fake = criterion(output, label)\n        errD_fake.backward(retain_graph=True)\n        D_G_z1 = output.mean().item()\n        \n        errD = errD_real + errD_fake\n        \n        optimizerD.step()\n\n        ############################\n        # (2) Update G network: maximize log(D(G(z)))\n        ###########################\n        netG.zero_grad()\n        inp_x_fake = {}\n        inp_x_fake['img']=fake_images\n        inp_x_fake['encoded'] = netE(images)\n        \n        label = torch.FloatTensor(np.random.uniform(low=0.895, high=0.999, size=(images.size(0)))).to(device)\n#         label.fill_(real_label)\n        output = netD(inp_x_fake).view(-1)\n        \n        errG = criterion(output, label) + 4*l1criterion(images,fake_images)\n        errG.backward(retain_graph=True)\n        D_G_z2 = output.mean().item()\n        optimizerG.step()\n\n        \n        netE.zero_grad()\n        inp_x_fake = {}\n        inp_x_fake['img']=fake_images\n        inp_x_fake['encoded'] = netE(images)\n        \n        label = torch.FloatTensor(np.random.uniform(low=0.895, high=0.999, size=(images.size(0)))).to(device)\n        output = netD(inp_x_fake).view(-1)\n\n        errE = criterion(output, label) + 4*l1criterion(images,fake_images)\n        errE.backward(retain_graph=True)\n        E_G_z2 = output.mean().item()\n        optimizerE.step()\n        \n        #################################_______STATS________###########################################\n        # Output training stats\n        if i % 50 == 0:\n            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tLoss_E: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n                  % (epoch, num_epochs, i, len(dataloader),\n                     errD.item(), errG.item(),errE.item(), D_x, D_G_z1, D_G_z2))\n\n        # Save Losses for plotting later\n        G_losses.append(errG.item())\n        D_losses.append(errD.item())\n        E_losses.append(errE.item())\n        \n        # Check how the generator is doing by saving G's output on fixed_noise\n#         if (iters % 50 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n#             netG.eval()\n#             with torch.no_grad():\n#                 fake = netG(fixed_noise).detach().cpu()\n#                 fake[:] = fake[:]*0.5 + 0.5\n#             img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n        del images\n        del inp_x_fake\n        del inp_x\n        del label\n        del output\n        torch.cuda.empty_cache()\n        iters += 1\n        \n        \n        \n        if i%500 ==0:\n            netE.eval()\n            netG.eval()\n            encoded_img = netE(valid_batch)\n            reconstructed_img = netG(encoded_img)\n            f, axarr = plt.subplots(num_images_to_show,2)\n            for i in range(num_images_to_show):\n                validimg = (valid_batch[i].cpu().detach().permute(1, 2, 0) * 0.5) + 0.5\n                rec_img = (reconstructed_img[i].cpu().detach().permute(1, 2, 0) *0.5 ) + 0.5\n                axarr[0].imshow(validimg)\n                axarr[1].imshow(rec_img)\n                f.set_figheight(20)\n                f.set_figwidth(20)\n            plt.show()","metadata":{"_uuid":"a8a9ee22-1470-4e66-915d-4b0a2d008ab8","_cell_guid":"5ba3c005-df29-4cfc-a638-b698da1973b9","collapsed":false,"execution":{"iopub.status.busy":"2024-03-02T14:31:16.058453Z","iopub.execute_input":"2024-03-02T14:31:16.058733Z","iopub.status.idle":"2024-03-02T18:47:51.301120Z","shell.execute_reply.started":"2024-03-02T14:31:16.058693Z","shell.execute_reply":"2024-03-02T18:47:51.300370Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Evaluating the model ...\")\nnetE.eval()\nnetG.eval()\ntot_img_size = IMG_WIDTH * IMG_HEIGHT * 3\n# print(\"Size reduction is : \"+ str(float(encode_size/tot_img_size)*100.0)+\" percent\")","metadata":{"_uuid":"81ca7d69-872b-425e-ad4e-18b71e01ef97","_cell_guid":"b73586ca-f8e1-4c40-bfe7-89d8de0e92f2","collapsed":false,"execution":{"iopub.status.busy":"2024-03-02T18:47:51.303247Z","iopub.execute_input":"2024-03-02T18:47:51.303572Z","iopub.status.idle":"2024-03-02T18:47:51.309075Z","shell.execute_reply.started":"2024-03-02T18:47:51.303515Z","shell.execute_reply":"2024-03-02T18:47:51.308118Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_dataset = ImageData(is_train=False)\nbatch_size=20\nvalid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\nvalid_batch = next(iter(valid_dataloader)).to(device)","metadata":{"_uuid":"6809ceac-bd62-4c7c-b52b-cacbbf3e228f","_cell_guid":"0a1e4150-16f9-4cf0-b31c-745221022743","collapsed":false,"execution":{"iopub.status.busy":"2024-03-02T18:47:51.310306Z","iopub.execute_input":"2024-03-02T18:47:51.310545Z","iopub.status.idle":"2024-03-02T18:47:51.648945Z","shell.execute_reply.started":"2024-03-02T18:47:51.310499Z","shell.execute_reply":"2024-03-02T18:47:51.648093Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(valid_batch.shape)\nencoded_img = netE(valid_batch)\nprint(encoded_img.shape)\nreconstructed_img = netG(encoded_img)\nprint(reconstructed_img.shape)","metadata":{"_uuid":"f55e3a87-fed7-4b54-95c4-52107de4ebb5","_cell_guid":"53983153-9d2a-4ee2-89bc-1f2af7bcdaeb","collapsed":false,"execution":{"iopub.status.busy":"2024-03-02T18:47:51.650453Z","iopub.execute_input":"2024-03-02T18:47:51.650772Z","iopub.status.idle":"2024-03-02T18:47:51.661721Z","shell.execute_reply.started":"2024-03-02T18:47:51.650715Z","shell.execute_reply":"2024-03-02T18:47:51.660955Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_images_to_show = 5\nf, axarr = plt.subplots(num_images_to_show,2)\nfor i in range(num_images_to_show):\n    validimg = (valid_batch[i].cpu().detach().permute(1, 2, 0) * 0.5) + 0.5\n    rec_img = (reconstructed_img[i].cpu().detach().permute(1, 2, 0) *0.5) + 0.5\n    axarr[i,0].imshow(validimg)\n    axarr[i,1].imshow(rec_img)\n    f.set_figheight(20)\n    f.set_figwidth(20)\nplt.show()","metadata":{"_uuid":"904fd525-bd69-43e8-af30-45e55511fc86","_cell_guid":"1accb9ca-7a38-453f-a0b8-cbe8af8b1a0a","collapsed":false,"execution":{"iopub.status.busy":"2024-03-02T18:47:51.664538Z","iopub.execute_input":"2024-03-02T18:47:51.664843Z","iopub.status.idle":"2024-03-02T18:47:53.641811Z","shell.execute_reply.started":"2024-03-02T18:47:51.664785Z","shell.execute_reply":"2024-03-02T18:47:53.640954Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(netE.state_dict(), \"netE\"+str(num_channels_in_encoder)+\".model\")\ntorch.save(netG.state_dict(), \"netG\"+str(num_channels_in_encoder)+\".model\")","metadata":{"_uuid":"9df6d0da-3efc-4a1b-887d-b47246cee9af","_cell_guid":"2ca760de-c1ce-468a-80fc-38adb24e2065","collapsed":false,"execution":{"iopub.status.busy":"2024-03-02T18:47:53.643362Z","iopub.execute_input":"2024-03-02T18:47:53.643652Z","iopub.status.idle":"2024-03-02T18:47:53.664287Z","shell.execute_reply.started":"2024-03-02T18:47:53.643604Z","shell.execute_reply":"2024-03-02T18:47:53.663379Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"a2771e31-2d3e-49c8-87af-080ec1c2ce1d","_cell_guid":"15d02e34-7146-4cad-834a-2f305aedbdb0","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}